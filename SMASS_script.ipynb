{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spectometric data from asteroids used for the smass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries and define functions.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "def correct_0(df):\n",
    "    \"\"\"cleans the data in various ways, like resetting the first row,\n",
    "    splitting columns and converting strings to ints or floats,\n",
    "    note that the column names arent necessary accurate.\"\"\"\n",
    "    zero = df.columns[0]\n",
    "    zero_df = pd.DataFrame([zero], columns=[zero])\n",
    "    df = pd.concat([zero_df, df])\n",
    "    df = df.reset_index(drop=True)\n",
    "    df['wavelength'] = df[zero].str[:-13]\n",
    "    df['wavelength'] = df['wavelength'].str.strip(' ')\n",
    "    df['reflectance'] = df[zero].str[-13:-6]\n",
    "    df['reflectance'] = df['reflectance'].str.strip(' ')\n",
    "    df = df.drop([zero], axis=1)\n",
    "    return df\n",
    "    \n",
    "\n",
    "def translate(df, data, name):\n",
    "    \"\"\"takes the individual spectra data for each asteroid and fits it into dataframe\n",
    "    with each possible filter as a parameter\"\"\" \n",
    "    data = correct_0(data)\n",
    "    df.loc[len(df)] = 0\n",
    "    df.loc[len(df)-1,'ast'] = name\n",
    "    for i in range(len(data['wavelength'])):\n",
    "        wave = data['wavelength'][i]\n",
    "        ref = data['reflectance'][i]\n",
    "        df.loc[len(df)-1, wave] = ref\n",
    "    return df\n",
    "    \n",
    "    \n",
    "def type_setter(name):\n",
    "    # THIS FUNCTION IS NOW OBSOLETE!\n",
    "    \"\"\"looks up (name) on wikipedia and if found returns the type of asteroid it is.\"\"\"\n",
    "    try:\n",
    "        result = wikipedia.search(name)[0]\n",
    "        ast = wikipedia.page(result)\n",
    "    except:\n",
    "        return 0\n",
    "    ast = ast.content\n",
    "    try:\n",
    "        if ast.find('X-type') != -1:\n",
    "            return 'X'\n",
    "        if ast.find('C-type') != -1:\n",
    "            return 'C'\n",
    "        if ast.find('S-type') != -1:\n",
    "            return 'S'\n",
    "        if ast.find('A-type') != -1:\n",
    "            return 'A'\n",
    "        if ast.find('B-type') != -1:\n",
    "            return 'B'\n",
    "        if ast.find('D-type') != -1:\n",
    "            return 'D'\n",
    "        if ast.find('Q-type') != -1:\n",
    "            return 'Q'\n",
    "        if ast.find('R-type') != -1:\n",
    "            return 'R'\n",
    "        if ast.find('T-type') != -1:\n",
    "            return 'T'\n",
    "        if ast.find('V-type') != -1:\n",
    "            return 'V'\n",
    "        if ast.find('K-type') != -1:\n",
    "            return 'K'\n",
    "        if ast.find('L-type') != -1:\n",
    "            return 'L'\n",
    "        if ast.find('O-type') != -1:\n",
    "            return 'O'\n",
    "    except: \n",
    "        return 0    \n",
    "\n",
    "def split_data(df):\n",
    "    \"\"\"splits the data into a dataset with S, C and Other as labels and another without S and C\"\"\"\n",
    "    df1 = df.copy()\n",
    "    df2 = df.copy()\n",
    "    for i in range(len(df1)):\n",
    "        if df1['type'][i] != 'S':\n",
    "            if df1['type'][i] != 'C':\n",
    "                 df1['type'][i] = 'Other'\n",
    "    for i in range(len(df2)):\n",
    "        if df2['type'][i] == 'S':\n",
    "            df2 = df2.drop([i], axis=0)\n",
    "        elif df2['type'][i] == 'C':\n",
    "            df2 = df2.drop([i], axis=0)\n",
    "    return df1, df2\n",
    "\n",
    "\n",
    "def ast_pred(features, xgbmodel, xgbmodel2):\n",
    "    \"\"\"cleans given dataset and uses both XGB models to predict the class of each row\"\"\"\n",
    "    features = features.drop(['ast', 'name', 'type'], axis=1)\n",
    "    p = xgbmodel.predict(features.values)\n",
    "    for i in range(len(p)):\n",
    "        if p[i] == 'Other':\n",
    "            p[i] = xgbmodel2.predict(features.loc[i:i].values)[0]\n",
    "    p = pd.DataFrame({'label': p})\n",
    "    return p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>name</th>\n",
       "      <th>number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>S</td>\n",
       "      <td>semiramis</td>\n",
       "      <td>584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>S</td>\n",
       "      <td>zuchong-zhi</td>\n",
       "      <td>1888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>X</td>\n",
       "      <td>happelia</td>\n",
       "      <td>578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label         name  number\n",
       "89      S    semiramis     584\n",
       "50      S  zuchong-zhi    1888\n",
       "117     X     happelia     578"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run this cell to test a pre-trained model. \n",
    "\n",
    "path = 'PATH/TO/FOLDER/1/'\n",
    "\n",
    "test = pd.read_csv(path+'test.csv')\n",
    "\n",
    "pred = ast_pred(test, xgbmodel=joblib.load(path+'xgbmodel.pkl'),\n",
    "                xgbmodel2=joblib.load(path+'xgbmodel2.pkl'))\n",
    "pred['name'] = test['name']\n",
    "pred['number'] = test['ast']\n",
    "\n",
    "pred.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- script inicialized --\n",
      "-- dataframe created --\n",
      "-- data loaded --\n",
      "-- taxonomy list set --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 18: expected 1 fields, saw 2\\nSkipping line 43: expected 1 fields, saw 2\\nSkipping line 73: expected 1 fields, saw 2\\nSkipping line 135: expected 1 fields, saw 2\\nSkipping line 258: expected 1 fields, saw 2\\nSkipping line 1429: expected 1 fields, saw 2\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- labels set --\n",
      "-- dataframe saved --\n",
      "-- test set ready --\n",
      "-- train set 1 ready --\n",
      "-- train set 2 ready --\n",
      "-- XGB 1 pickled --\n",
      "-- XGB 2 pickled --\n",
      "-- accuracy:  87.21804511278195 --\n",
      "-- script finalized --\n"
     ]
    }
   ],
   "source": [
    "# run this cell to clean the spectometry data and train a model from scratch. \n",
    "\n",
    "path = 'PATH/TO/FOLDER/2/' \n",
    "\n",
    "print('-- script inicialized --')\n",
    "\n",
    "path_to_folders = path + 'EAR_A_I0028_4_SBN0001_SMASSII_V1_0/data'\n",
    "folders = os.listdir(path_to_folders)\n",
    "folders = folders[2:]\n",
    "tables = []\n",
    "df = pd.DataFrame(columns=['wavelength', 'reflectance'])\n",
    "\n",
    "# append data from all files to get every unique wavelength.\n",
    "\n",
    "for folder in folders:\n",
    "    files = os.listdir(path_to_folders+'/'+folder)\n",
    "    for file in files:\n",
    "        if '.tab' in file:\n",
    "            tables.append(folder+'/'+file)\n",
    "\n",
    "for file in tables:\n",
    "    data = pd.read_csv(path_to_folders+'/'+file)\n",
    "    data = correct_0(data)\n",
    "    df = df.append(data)\n",
    "    \n",
    "    \n",
    "df = df.reset_index(drop=True)\n",
    "    \n",
    "\n",
    "features = df['wavelength'].unique()\n",
    "features = np.sort(features)\n",
    "col = np.append(['ast'],features)\n",
    "smass_dataframe = pd.DataFrame(columns=col)\n",
    "\n",
    "print('-- dataframe created --')\n",
    "\n",
    "# load the data from the individual asteroid files.\n",
    "\n",
    "for file in tables:\n",
    "    data = pd.read_csv(path_to_folders+'/'+file)\n",
    "    smass_dataframe = translate(smass_dataframe, data, file)\n",
    "    \n",
    "    \n",
    "    \n",
    "smass_dataframe['ast'] = pd.Series(smass_dataframe.ast.values.flatten()).str.split('/')\n",
    "smass_dataframe['ast'] = pd.Series(smass_dataframe.ast.values.flatten()).str[1]\n",
    "smass_dataframe['ast'] = pd.Series(smass_dataframe.ast.values.flatten()).str.split('_')\n",
    "smass_dataframe['ast'] = pd.Series(smass_dataframe.ast.values.flatten()).str[0]\n",
    "\n",
    "\n",
    "\n",
    "smass_dataframe['type'] = 0\n",
    "smass_dataframe['name'] = 0\n",
    "\n",
    "print('-- data loaded --')\n",
    "\n",
    "# prepare and clean the taxonomy file from which labels will be extracted.\n",
    "\n",
    "\n",
    "tax = pd.read_csv(path+'taxonomy10.csv', error_bad_lines=False)\n",
    "tax = correct_0(tax)\n",
    "tax['smass type'] = tax['wavelength'].str[-32]\n",
    "tax['tholen type'] = tax['wavelength'].str[-16]\n",
    "tax['??? type'] = tax['wavelength'].str[-8]\n",
    "tax['last resort type'] = tax['wavelength'].str[-24]\n",
    "tax['ast'] = tax['wavelength'].str[:19]\n",
    "tax['ast'] = tax['ast'].str.replace(' ', '')\n",
    "tax['ast'] = tax['ast'].str.lower()\n",
    "tax['number'] = tax['ast'].str.strip('abcdefghijklmnopqrstuvwxyz-')\n",
    "tax['name'] = tax['ast'].str.strip('0123456789-')\n",
    "tax = tax.drop(['ast'], axis=1)\n",
    "tax = tax.drop(['wavelength', 'reflectance'], axis=1)\n",
    "\n",
    "print('-- taxonomy list set --')\n",
    "\n",
    "# use the new taxonomy dataframe to set labels.\n",
    "# the nested for loop is important so that each row in smass_dataframe is compared to every row in tax.\n",
    "\n",
    "for i in range(len(smass_dataframe)):\n",
    "    for j in range(len(tax)):\n",
    "        if str(smass_dataframe['ast'].loc[i]) == tax['number'][j]:\n",
    "            smass_dataframe['name'].iloc[i] = tax['name'][j]\n",
    "            if tax['smass type'][j] != '-':\n",
    "                smass_dataframe['type'].iloc[i] = tax['smass type'][j]\n",
    "            else:\n",
    "                if tax['tholen type'][j] != '-':\n",
    "                    smass_dataframe['type'].iloc[i] = tax['tholen type'][j]\n",
    "                else:\n",
    "                    if tax['??? type'][j] != '-':\n",
    "                        smass_dataframe['type'].iloc[i] = tax['??? type'][j]\n",
    "                    else:\n",
    "                        smass_dataframe['type'].iloc[i] = tax['last resort type'][j]\n",
    "                    \n",
    "smass_dataframe = smass_dataframe.replace(['-', ' '], 0)\n",
    "for i in range(len(smass_dataframe)):\n",
    "    if smass_dataframe['type'][i] == 0:\n",
    "        smass_dataframe = smass_dataframe.drop([i], axis=0)\n",
    "\n",
    "print('-- labels set --')\n",
    "\n",
    "\n",
    "# save the dataframe\n",
    "\n",
    "smass_dataframe.to_csv(path+'smass.csv', index=False)\n",
    "\n",
    "print('-- dataframe saved --')\n",
    "\n",
    "\n",
    "# split the data\n",
    "\n",
    "train, test = train_test_split(smass_dataframe, test_size=0.1)\n",
    "train = train.reset_index(drop=True)                              \n",
    "test = test.reset_index(drop=True)\n",
    "test.to_csv(path+'test.csv', index=False)\n",
    "\n",
    "print('-- test set ready --')\n",
    "                          \n",
    "train1, train2 = split_data(train)\n",
    "\n",
    "\n",
    "# prepare data for the first xgb classifier\n",
    "                                  \n",
    "train_labels = train1.pop(\"type\")\n",
    "                                  \n",
    "                                  \n",
    "train_ast = train1.pop(\"ast\")\n",
    "\n",
    "train_name = train1.pop(\"name\")\n",
    "\n",
    "train_features = train1.values\n",
    "\n",
    "print('-- train set 1 ready --')\n",
    "\n",
    "\n",
    "# prepare data for the second xgb classifier\n",
    "\n",
    "train_labels2 = train2.pop(\"type\")\n",
    "\n",
    "\n",
    "train_ast2 = train2.pop(\"ast\")\n",
    "\n",
    "train_name2 = train2.pop(\"name\")\n",
    "\n",
    "train_features2 = train2.values\n",
    "\n",
    "print('-- train set 2 ready --')\n",
    "\n",
    "\n",
    "# train models\n",
    "\n",
    "xgbmodel = xgb.XGBClassifier()\n",
    "xgbmodel.fit(train_features, train_labels)\n",
    "joblib.dump(xgbmodel, path+'xgbmodel.pkl', compress=True)\n",
    "print('-- XGB 1 pickled --')\n",
    "\n",
    "xgbmodel2 = xgb.XGBClassifier()\n",
    "xgbmodel2.fit(train_features2, train_labels2)\n",
    "joblib.dump(xgbmodel2, path+'xgbmodel2.pkl', compress=True)\n",
    "print('-- XGB 2 pickled --')\n",
    "\n",
    "\n",
    "# test the model\n",
    "\n",
    "preds = ast_pred(test, xgbmodel=joblib.load(path+'xgbmodel.pkl'),\n",
    "                xgbmodel2=joblib.load(path+'xgbmodel2.pkl'))\n",
    "\n",
    "X = 0\n",
    "O = 0\n",
    "\n",
    "for i in range(len(test)):\n",
    "    if preds['label'][i] == test['type'][i]:\n",
    "        O += 1\n",
    "    else:\n",
    "        X += 1\n",
    "        \n",
    "acc = O*100/len(test) \n",
    "print('-- accuracy: ', acc,'--')\n",
    "\n",
    "\n",
    "print('-- script finalized --')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
